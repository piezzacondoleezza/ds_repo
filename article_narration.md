# Краткое содержание статьи Calvin: Fast Distributed Transactions for Partitioned Database Systems

link: http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf

некоторые вольности перевода, которые, к сожалению, выродились:
* lock'и = блокировки
* transactions = обращения/иногда запросы
* scheduler = шедулер/планировщик 


**Вступление**

В данный момент наблюдаются активные попытки уйти от поддержки стандартных ACID-databases. Некоторые из них не поддерживают transactional support(такие как MongoDB, CoachDB и проч.), другие поддерживают только изменения в одной-единственной строке, третьи поддерживают доступ только к ограниченному небольшому набору данных в одном запросе. Главная проблема почему данные виды баз данных не поддерживают полностью парадигму ACID-transactions - это попытка предоставить линейную внешнюю мастштабируемость при работе с ними. Отсустствие данной поддержки значительно упрощает задачу сборки проектов, но, безусловно, при сильной связности проекта, проблемы с build-perfomance остаются на совести пользователя. В свою очередь, Calvin был спроектирован так, чтобы он мог предоставлять пользователю возможность работы с non-transactional базой данных, превращая ее в систему баз данных без совместного использования (почти) линейно масштабируемую, которая обеспечивает высокую доступность и полные ACID-transactions. Так же он имеет в себе дополнительный слой, содержащий шедуллинг распределенных запросов, а также репликацию данных и общение по сети. Но главной технической идеей является детерменированный locking mechanism, который позволяет исключить такую вещь как distributed commits.

**Стоимость распределенных вычислений**

Изначально запросы к базам данных выполнялись в следующей манере, называемой System-R* style:

Они увеличивают latency of transactions как соглашение между всеми машинами, участвующими в системе во время коммита, для того, чтобы обеспечить атомарность и durability всей системы. Для обеспечения устойчивости, блокировки должны быть проставлены на все время действия данного протокола, что обычно представляет собой двухэтапный коммит. Главная проблема удержания блокировки в том, что машины должны общаться между собой по сети, что занимает значительно больше времени, чем выполнение какой-то логики на каждой из них локально. Так же, элементы к которым часто обращаются в запросах, могут попадать в данные блокировки, и это значительно ухудшает пропускную способность сети. Для того, чтобы улучшить данную часть логики нашей БД, можно попробовать сделать стандартный прием экономии CPU и улучшения пропускной способности сети - использовать блокировки не на единственном commit'e, а сразу на batch-е из них, но, увы, latency это значительно не улучшит. Однако, если мы разрешим распределенные запросы, тогда могут возникнуть дедлоки и мы снова из-за этого можем получить ощутимую просадку по пропускной способности и cpu, ибо некоторые ядра будут просто простаивать. 

**Пару слов о консистентности и репликации**

Почти любая база данных использует репликацию для гарантии сохранности данных, этот случай не исключение. Во многих базах данных наблюдается тенденция снижения гарантии последовательности/согласованности(консистентности) данных, для того чтобы достичь постоянной доступности для пользователя 24/7 даже при разделении сети, для этого требуется понижать гарантии консистентности данных. Однако недавно, данный тренд поменялся в обратную сторону, к strong-consistency. Так же заметим, что обновления реплик, в таких таблицах очень тесно сопряжено с соединениями по сети между репликами, и из-за этого так же latency может быть сильно увеличено в связи с тем, что некоторые запросы еще и могут ретраиться, из-за чего стоимость может значительно вырасти. 

**Подходы Calvin**

Подход Кальвина к достижению недорогих распределенных транзакций и синхронной репликации заключается в следующем:

Машины должны договориться о том, как обрабатывать конкретную транзакцию, за пределами транзакционных границ, начать выполнение после блокировки

После того, как соглашение о том, как обрабатывать транзакцию сделано - транзакция должна быть выполнена в любом случае.


Если узел выходит из строя, он может восстановиться из репликации, для которой все выполняялось паралелльно, или возможно восстановление истории для изначального узла системы
Оба этих плана должны подразумевать, что они идентичны, для сохранения консистентности данных. 
Чтобы поддержать эту гарантию детерминированности, и одновременно максимизировать распараллеливание при выполнении транзакций, Calvin использует протокол детерминированной блокировки, основанный на протоколе, который мы представили в предыдущей работе.

Так как все узлы Calvin сами должны договориться о плане и следовать ему, то строгий commit protocol в данном случае не нужен, и количество ошибок при распределенных транзакциях уменьшиться, что позволяет улучшить пропускную способность и сделать масштабирование почти линейным, несмотря на наличие multipartition transactions.

Благодаря экспериментам было выяснено, что Calvin при высоких рабочих нагрузках значительно превосходит традиционные схемы распределенных баз данных. 

Основные вклады этой статьи заключаются в следующем:

* Дизайн планирования запросов и репликации данных слой, который преобразует non-transactional систему хранения в почти линейно масштабируемую систему с полным ACID-transactions и strong consistency. 
* На практике реализован легко масштабируемый параллелизм, без конкретной точки отказа
* Механизм предварительной выборки данных, чтобы работать только с нужными данными заранее выбранными на диске
* Удобная и быстрая схема контрольных точек, которая вместе с гарантией детерменированности Calvin.

**Детерминированная база данных**

В традиционных системах распределенных баз данных необходим протокол для доставки информации в долговременное хранилище атомарно, способ — либо все узлы, участвующие в транзакции, соглашаются «зафиксировать» свои локальные изменения, либо ни один из них не делает этого. События, препятствующие данному узлу чтобы зафиксировать изменения(и, следовательно, вызвать прерывание всего запроса) делятся на две категории: недетерминированные события(такие как сбои внутри узлов) и детерминированные события(такие как логика запроса, вызывающая прерывание, если, скажем, не хватает ресурсов).

Не существует фундаментальной причины, по которой запрос должен прерываться по причине какого-либо недетерминированного события - когда системы выбирают прерывать запросы из-за внешних событий, это связано со стечением обстоятельств на практике. В итоге остальные узлы остаются ждать, пока на заболевшем узле проблема решится(например из-за аппаратного сбоя)

Если есть узел-реплика, выполняющий точно такие же операции параллельно отказавшему узлу, то другие узлы, которые зависят от мертвого узла для выполнения запроса, то не нужно ждать пока отказавший узел восстановится, легче просто начать делать запросы к узлу реплики, для любых необходимых данных, ибо реплика сможет завершить начатый процесс, а восстановившийся узел сможет доделать начатую работу.

Соответственно остается только проблема того, что реплика должна пройти через те же состояния, что и изначальный узел, вышедший из строя в случайном месте. Если мы будем делать это синхронно, то затраты ресурсов возрастут в несколько раз, что ясно дело, не очень хорошо. Поэтому, детерменированные базы данных синхронно реплицируют батчи запросов между узлами. Так же, в отличие от стандартных баз данных, в данном месте используется продвинутый уровень управления параллелизмом, благодаря чему мы можем сохранять порядок обращений к бд и сохранять эквивалентность состояний реплик. Таким образом мы устраняем проблемы прерываний обращений к базе данных из-за недетерменированных событий довольно дешево. 

**Архитектура Calvin**

* Упорядочивающий слой: преобразует какой-то набор входных данных(возможно пришедших одновременно) в какую-то последовательность, для того чтобы обеспечить согласованность реплик.
* Слой планировщик(шедулер): делает определенную схему обработки данных, проставляя нужные блокировки, проще говоря строит многопоточный граф обработки обращений к базе данных над каким-то thread-pool. 
* Хранилище: содержит в себе физические данные. Запросы к данным Calvin используют простой CRUD интерфейс, использующие который другие базы данных, легко могут быть подстроены в Calvin 

Картиночка из оригинала для удобства понимания:

![Иллюстрация](https://github.com/piezzacondoleezza/ds_repo/raw/main/image.png)


**Некоторый разбор уровней архитектуры**

Для репликации последовательностей мы разбиваем все время на определенные промежутки, в конце каждого из которых мы собираем все запросы и упорядочиваем их. Затем отправляем данную последовательность планировщику tuple вида (SequencerNodeId, SequencerInstant, TransactionsInputs), это дает зависимым шедуллерам возможность удобно обрабатывать данные. Не вдаваясь в глубокие подробности, мы можем реплицировать состояния синхронно и асинхронно, что может дать нам различные плюсы и минусы. Но с помощью асинхронного подхода мы можем добиться гораздо более хорошего latency в среднем, но расплачиваться очень высокой стоимостью ошибки.

Для планировщика у нас в каждый момент должен соблюдаться один из следующих инвариантов: 
* Для любой пары событий A и B, если оба из них подразумевают какую-то блокировку на R, тогда если A идет раньше B в последовательности, тогда A обязана запросить блокировку на R перед B. Однако важно заметить, что на практике все локи запрашиваются в единственном потоке, что сильно упрощает поддержку инварианта.
* Планировщик должен обеспечить локами запросы ровно в том порядке, в котором они их запрашивают.

Далее, когда стадия блокировок прошла успешно, запускаются worker-threads, и для каждого события выделяется свой воркер. Каждое из них должно пройти 5 стадий:
* Анализ на чтение/запись ли этот запрос
* Стадия локального чтения(все к чему локально имеет доступ воркер)
* Настройка чтения удаленно находящихся данных(самым логичным образом, мы смотрим какие данные нужны каждому из воркеров и отправляем их вместе)
* Сохранение прочитанных удаленно находящихся данных
* Логика исполнения запросов и запись в базу данных

Так же хочется заметить, что зависимые запросы в базе данных не разрешены по причине того, что нам важно знать всю нужную информацию о запросе на стадии упорядочивания.

**Calvin как база данных на диске**

Для работы на диске Calvin некоторым образом избегает определенного недостатка детерменизма в виде строгого порядка и старается переместить как можно больше сложных вычислений на как можно более ранее время, чтобы избежать их подсчета после проставления большого числа блокировок. Если компонент упорядочивания получает довольно тяжелый запрос, который может вызвать остановку диска - он искусственно вызывает задержку(не более стандартной) и  отправляет всем соответствующим компонентам хранилища запрос на "подогрев" записей на диске. И соответственно переносит все наличествующие данные, которые следует переместить на диск - переносит на диск. Были проведены эксперименты, по итогу которых выяснилось, что преимущественно запросы не затрагивают перенос на диск(около 0,9%) среди десяти тысяч, а значит, как известно, сильных просадок по latency быть не должно. 

Поскольку далее идет анализ бенчмарок, мне кажется, ничто не опишет их лучше, чем графики:






